{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553fa4ee",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "221d56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# PyTorch ecosystem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# Preprocessing & metrics\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ab7b5",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a638b49",
   "metadata": {},
   "source": [
    "#### Collect tickers (archaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "834d75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import a list of tickers to include in the dataset\n",
    "# # Wikipedia URL for S&P 500 Tickers\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "# Pull the table from Wikipedia\n",
    "sp500_tables = pd.read_html(response.text)\n",
    "# Figure out a way to only keep the \"Symbols\" column\n",
    "sp500_df = sp500_tables[1]\n",
    "tickers = sp500_df['Symbol'].str.replace('.', '-').to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269f8e9",
   "metadata": {},
   "source": [
    "#### Access SimFin API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "da9970dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simfin as sf\n",
    "from simfin.names import *\n",
    "\n",
    "# FinancialModelingPrep API Key\n",
    "sf.set_api_key('3710f0ef-cd08-4273-ad23-6caf78ea7396')\n",
    "\n",
    "# Set local data directory\n",
    "DATA_DIR = '/home/lukeditzler/projects/pytorch/examples/Financial_Markets_Model/data/simfin_data/'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Set local data directory (where bulk downloads will be stored)\n",
    "sf.set_data_dir('simfin_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "705cdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-balance-quarterly\" on disk (0 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-income-quarterly\" on disk (0 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-cashflow-quarterly\" on disk (0 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-shareprices-daily\" on disk (0 days old).\n",
      "- Loading from disk ... Done!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load quarterly statements ---\n",
    "df_balance_q = sf.load_balance(variant='quarterly', market='us')\n",
    "df_income_q = sf.load_income(variant='quarterly', market='us')\n",
    "df_cash_q = sf.load_cashflow(variant='quarterly', market='us')\n",
    "\n",
    "# Bring index into columns\n",
    "df_balance_q = df_balance_q.reset_index()\n",
    "df_income_q = df_income_q.reset_index()\n",
    "df_cash_q = df_cash_q.reset_index()\n",
    "\n",
    "# --- 2. Load daily share prices ---\n",
    "df_prices = sf.load_shareprices(variant='daily', market='us')\n",
    "df_prices = df_prices.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045054e5",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a7391b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1. Merge quarterly fundamentals\n",
    "# ----------------------------------------------------------\n",
    "def merge_quarterly_fundamentals(df_income_q, df_balance_q, df_cash_q):\n",
    "    start = time.perf_counter()\n",
    "    print(\"\\n--- MERGING QUARTERLY FUNDAMENTALS ---\")\n",
    "\n",
    "    key_cols = [\"Ticker\", \"Report Date\"]\n",
    "\n",
    "    # Ensure datetime for merge consistency\n",
    "    for df in [df_income_q, df_balance_q, df_cash_q]:\n",
    "        df[\"Report Date\"] = pd.to_datetime(df[\"Report Date\"])\n",
    "\n",
    "    # Merge\n",
    "    fundamentals = (\n",
    "        df_income_q.merge(df_balance_q, on=key_cols, how=\"left\")\n",
    "                   .merge(df_cash_q, on=key_cols, how=\"left\")\n",
    "    )\n",
    "\n",
    "    print(f\"Before cleaning shape: {fundamentals.shape}\")\n",
    "\n",
    "    # --- CLEAN DUPLICATE SUFFIX COLS HERE ---\n",
    "    fundamentals = drop_suffix_duplicate_columns(fundamentals)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Fundamentals merged in {elapsed:.2f} seconds\")\n",
    "    print(f\"After cleaning shape: {fundamentals.shape}\")\n",
    "\n",
    "    return fundamentals\n",
    "\n",
    "def drop_suffix_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Keeps clean column names (without suffixes) and removes all suffixed versions\n",
    "    like `_x`, `_y`, `_BAL`, `_CASH`, etc.\n",
    "    \n",
    "    If no clean version exists (rare), one suffixed version is kept.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    keep_cols = set()\n",
    "\n",
    "    # Identify base names and available clean columns\n",
    "    base_to_clean = {}\n",
    "    base_to_suffixes = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        parts = col.split('_')\n",
    "        \n",
    "        # Heuristic: suffixes are short (x,y,BAL,CASH)\n",
    "        is_suffix = len(parts) > 1 and parts[-1].upper() in {\"X\", \"Y\", \"BAL\", \"CASH\"}\n",
    "\n",
    "        base = parts[0]\n",
    "\n",
    "        if not is_suffix:\n",
    "            base_to_clean[base] = col\n",
    "        else:\n",
    "            base_to_suffixes.setdefault(base, []).append(col)\n",
    "\n",
    "    # Decide what to drop\n",
    "    for base, suffixed_cols in base_to_suffixes.items():\n",
    "        if base in base_to_clean:\n",
    "            # Clean column exists → drop *all* suffixed versions\n",
    "            cols_to_drop.extend(suffixed_cols)\n",
    "        else:\n",
    "            # No clean version → keep ONE suffixed version, drop the rest\n",
    "            keep_cols.add(suffixed_cols[0])\n",
    "            cols_to_drop.extend(suffixed_cols[1:])\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    print(\"\\nDropped duplicate-suffix columns:\")\n",
    "    for c in cols_to_drop:\n",
    "        print(\"  -\", c)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Merge fundamentals with daily prices (corrected)\n",
    "# ----------------------------------------------------------\n",
    "def merge_fundamentals_with_prices(fundamentals, df_prices):\n",
    "    start = time.perf_counter()\n",
    "    print(\"\\n--- MERGING FUNDAMENTALS + PRICES ---\")\n",
    "\n",
    "    # Convert dates\n",
    "    fundamentals[\"Report Date\"] = pd.to_datetime(fundamentals[\"Report Date\"])\n",
    "    df_prices[\"Date\"] = pd.to_datetime(df_prices[\"Date\"])\n",
    "\n",
    "    # Sort both DataFrames\n",
    "    fundamentals = fundamentals.sort_values(['Ticker', 'Report Date']).reset_index(drop=True)\n",
    "    df_prices = df_prices.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # Merge per ticker using merge_asof\n",
    "    merged_list = []\n",
    "    for ticker in fundamentals['Ticker'].unique():\n",
    "        fund_ticker = fundamentals[fundamentals['Ticker'] == ticker].copy()\n",
    "        price_ticker = df_prices[df_prices['Ticker'] == ticker].copy()\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            fund_ticker,\n",
    "            price_ticker,\n",
    "            left_on='Report Date',\n",
    "            right_on='Date',\n",
    "            direction='backward'\n",
    "        )\n",
    "        merged_list.append(merged)\n",
    "\n",
    "    master = pd.concat(merged_list, ignore_index=True)\n",
    "\n",
    "    # Optional: rename price date for clarity\n",
    "    master = master.rename(columns={'Date':'Price Date'})\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Master DF merged in {elapsed:.2f} seconds\")\n",
    "    print(f\"Master DF shape: {master.shape}\")\n",
    "\n",
    "    return master\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Drop sparse columns\n",
    "# ----------------------------------------------------------\n",
    "def drop_sparse_columns(df, min_non_null_pct=0.50): # default 50% data\n",
    "    start = time.perf_counter()\n",
    "    print(\"\\n--- DROPPING SPARSE COLUMNS ---\")\n",
    "\n",
    "    threshold = len(df) * min_non_null_pct\n",
    "\n",
    "    # Columns above threshold → keep\n",
    "    good_cols = [c for c in df.columns if df[c].count() >= threshold]\n",
    "\n",
    "    # Columns below threshold → drop\n",
    "    dropped_cols = [c for c in df.columns if c not in good_cols]\n",
    "\n",
    "    df_clean = df[good_cols]\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Sparse-column cleanup completed in {elapsed:.2f} seconds\")\n",
    "    print(f\"Before: {df.shape} → After: {df_clean.shape}\")\n",
    "\n",
    "    if dropped_cols:\n",
    "        print(\"\\nDropped sparse columns:\")\n",
    "        for col in dropped_cols:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"No columns dropped.\")\n",
    "\n",
    "\n",
    "        df_balance_q,\n",
    "    df_cash_q\n",
    "\n",
    "    # Remove columns that are mostly descriptive/metadata\n",
    "    df_clean = df_clean.rename(columns={'Ticker_x': 'Ticker'})\n",
    "    meta_cols = ['Ticker_y','SimFinId_x','SimFinId_y', \"Currency\",\"Fiscal Year\",\"Fiscal Period\",\n",
    "                \"Publish Date\",\"Restated Date\"]\n",
    "    df_clean = df_clean.drop(columns=[c for c in meta_cols if c in df_clean.columns])\n",
    "\n",
    "    # Remove rows that didn't connect to price data (~4%)\n",
    "    price_cols = ['Open','High','Low','Close','Adj. Close','Volume','Shares Outstanding']\n",
    "    df_clean = df_clean.dropna(subset=price_cols)\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d47308",
   "metadata": {},
   "source": [
    "#### Run Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MERGING QUARTERLY FUNDAMENTALS ---\n",
      "Before cleaning shape: (53186, 82)\n",
      "\n",
      "Dropped duplicate-suffix columns:\n",
      "  - SimFinId_x\n",
      "  - SimFinId_y\n",
      "  - Currency_x\n",
      "  - Currency_y\n",
      "  - Fiscal Year_x\n",
      "  - Fiscal Year_y\n",
      "  - Fiscal Period_x\n",
      "  - Fiscal Period_y\n",
      "  - Publish Date_x\n",
      "  - Publish Date_y\n",
      "  - Restated Date_x\n",
      "  - Restated Date_y\n",
      "  - Shares (Basic)_x\n",
      "  - Shares (Basic)_y\n",
      "  - Shares (Diluted)_x\n",
      "  - Shares (Diluted)_y\n",
      "  - Depreciation & Amortization_y\n",
      "Fundamentals merged in 0.05 seconds\n",
      "After cleaning shape: (53186, 65)\n",
      "\n",
      "--- MERGING FUNDAMENTALS + PRICES ---\n",
      "Master DF merged in 489.27 seconds\n",
      "Master DF shape: (53186, 76)\n"
     ]
    }
   ],
   "source": [
    "# Combine Income Statement, Balance Sheet, and Cash Flow DataFrames\n",
    "fundamentals = merge_quarterly_fundamentals(\n",
    "    df_income_q,\n",
    "    df_balance_q,\n",
    "    df_cash_q\n",
    ")\n",
    "\n",
    "# Align each quarterly report with the last available stock price\n",
    "master_df = merge_fundamentals_with_prices(\n",
    "    fundamentals,\n",
    "    df_prices\n",
    ")\n",
    "\n",
    "# Remove any columns where less than 75% of the rows have data\n",
    "master_df = drop_sparse_columns(master_df, min_non_null_pct=0.75)\n",
    "\n",
    "# Save to CSV\n",
    "master_df.to_csv(\"historical_finance_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6e9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
